# apis/ai_route.py  
import uuid
from flask import Blueprint, request, jsonify, current_app as app
from concurrent.futures import ThreadPoolExecutor
import threading
import requests
import traceback
from datetime import datetime
from app import global_logger
from configs import format, config
import time
import json
from dbutils.pooled_db import PooledDB
from services.notification_service import send_email_task
import pymysql

ai_route = Blueprint('ai_route', __name__)

#è·å–æ•°æ®åº“è¿æ¥æ± 
mysql_pool=PooledDB(
    creator=pymysql,
    maxconnections=6,
    mincached=2,
    maxcached=5,
    maxshared=3,
    blocking=True,
    maxusage=None,
    setsession=[],
    ping=0,
    host=config.MYSQL_HOST,
    port=config.MYSQL_PORT,
    user=config.MYSQL_USER,
    password=config.MYSQL_PASSWORD,
    database=config.MYSQL_DATABASE,
    cursorclass=pymysql.cursors.DictCursor
)


def execute_query(sql, params=None):
    """æ‰§è¡ŒSQLæŸ¥è¯¢ï¼ˆINSERT, UPDATE, DELETEï¼‰"""
    conn = None
    cursor = None
    try:
        conn = mysql_pool.connection()
        cursor = conn.cursor()

        if params:
            cursor.execute(sql, params)
        else:
            cursor.execute(sql)

        conn.commit()
        return cursor.lastrowid

    except Exception as e:
        if conn:
            conn.rollback()
        global_logger.error(f"æ‰§è¡ŒSQLå¤±è´¥: {e}")
        global_logger.error(f"SQL: {sql}")
        global_logger.error(f"å‚æ•°: {params}")
        raise e
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()


def fetch_one(sql, params=None):
    """è·å–å•æ¡è®°å½•"""
    conn = None
    cursor = None
    try:
        conn = mysql_pool.connection()
        cursor = conn.cursor()

        if params:
            cursor.execute(sql, params)
        else:
            cursor.execute(sql)

        result = cursor.fetchone()
        return result

    except Exception as e:
        global_logger.error(f"æŸ¥è¯¢å•æ¡è®°å½•å¤±è´¥: {e}")
        global_logger.error(f"SQL: {sql}")
        global_logger.error(f"å‚æ•°: {params}")
        raise e
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()


def fetch_all(sql, params=None):
    """è·å–å¤šæ¡è®°å½•"""
    conn = None
    cursor = None
    try:
        conn = mysql_pool.connection()
        cursor = conn.cursor()

        if params:
            cursor.execute(sql, params)
        else:
            cursor.execute(sql)

        results = cursor.fetchall()
        return results

    except Exception as e:
        global_logger.error(f"æŸ¥è¯¢å¤šæ¡è®°å½•å¤±è´¥: {e}")
        global_logger.error(f"SQL: {sql}")
        global_logger.error(f"å‚æ•°: {params}")
        raise e
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()
@ai_route.route('/apis/ai_route/ping', methods=['GET'])
def ping():
    global_logger.info("æ”¶åˆ°Pingè¯·æ±‚")
    return jsonify({
        "code": 200,
        "message": "pong",
        "timestamp": datetime.now().isoformat()
    })

@ai_route.route('/apis/ai_route/generate-testcases', methods=['POST'])
def generate_testcases():
    """AIç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å¹¶é€šè¿‡HTTPè°ƒç”¨ä¿å­˜"""
    global_logger.info("=== å¼€å§‹AIç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ ===")

    try:
        # å‚æ•°éªŒè¯
        interface_data = request.get_json()
        if not interface_data:
            response = format.resp_format_failed.copy()
            response["message"] = "è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©º"
            return response

            # åŸºç¡€å¿…éœ€å‚æ•°ï¼ˆæ”¾å®½è¦æ±‚ï¼‰
        required_fields = ['app_id', 'name', 'url', 'method']
        missing_fields = [field for field in required_fields if not interface_data.get(field)]
        if missing_fields:
            response = format.resp_format_failed.copy()
            response["message"] = f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(missing_fields)}"
            return response

            # è®¾ç½®é»˜è®¤å€¼
        interface_data.setdefault('interface_id', interface_data.get('app_id') + '_default_if')
        interface_data.setdefault('creator_id', 'ai_system')
        interface_data.setdefault('creator_name', 'AIç³»ç»Ÿ')
        interface_data.setdefault('category', interface_data.get('category', 'ç”¨æˆ·ç®¡ç†'))
        interface_data.setdefault('description', interface_data.get('description', 'AIç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹'))

        global_logger.info(f"å¤„ç†æ¥å£: {interface_data['name']} ({interface_data['method']} {interface_data['url']})")

        # æ­¥éª¤1ï¼šåˆ›å»ºAIæ‰¹æ¬¡
        global_logger.info("åˆ›å»ºAIæµ‹è¯•æ‰¹æ¬¡...")
        batch_id = str(uuid.uuid4()).replace('-', '')
        batch_name = interface_data.get('batch_name',
                                        f"AIæµ‹è¯•-{interface_data['name']}-{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        current_time = int(time.time())

        try:
            # è¿æ¥æ•°æ®åº“åˆ›å»ºæ‰¹æ¬¡
            conn = mysql_pool.connection()
            cursor = conn.cursor()

            batch_sql = """  
                INSERT INTO api_test_batch (  
                    id, app_id, name, total_cases, passed_cases, failed_cases,  
                    executor_name, create_time, status, ai_generated  
                ) VALUES (  
                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s  
                )  
            """

            cursor.execute(batch_sql, [
                batch_id,
                interface_data['app_id'],
                batch_name,
                0,  # total_casesï¼Œç¨åæ›´æ–°
                0,  # passed_cases
                0,  # failed_cases
                interface_data['creator_name'],
                current_time,
                1,  # statusï¼šåˆ›å»ºä¸­
                1  # ai_generated = 1
            ])

            conn.commit()
            cursor.close()
            conn.close()

            global_logger.info(f"AIæ‰¹æ¬¡åˆ›å»ºæˆåŠŸï¼ŒID: {batch_id}")

        except Exception as e:
            global_logger.error(f"åˆ›å»ºAIæ‰¹æ¬¡å¤±è´¥: {str(e)}")
            response = format.resp_format_failed.copy()
            response["message"] = f"åˆ›å»ºAIæ‰¹æ¬¡å¤±è´¥: {str(e)}"
            return response

            # æ­¥éª¤2ï¼šè°ƒç”¨AIç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        global_logger.info("è°ƒç”¨AIæœåŠ¡ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹...")
        from services.ai_service import get_ai_service
        ai_service = get_ai_service()
        global_logger.info(f"AIæœåŠ¡å¯¹è±¡: {ai_service}")

        testcases = ai_service.generate_testcases(interface_data, count=10)
        global_logger.info(f"AIç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹: {testcases}")

        # ä¿®æ”¹è¿™é‡Œï¼šå¦‚æœAIç”Ÿæˆå¤±è´¥ï¼Œç›´æ¥è·³è¿‡ï¼Œä¸æŠ¥é”™
        if not testcases:
            global_logger.warning("AIæœåŠ¡è¿”å›ç©ºæµ‹è¯•ç”¨ä¾‹åˆ—è¡¨ï¼Œè·³è¿‡ä¿å­˜æ­¥éª¤")

            # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€ä¸ºå®Œæˆï¼Œä½†æµ‹è¯•ç”¨ä¾‹æ•°ä¸º0
            try:
                conn = mysql_pool.connection()
                cursor = conn.cursor()
                update_batch_sql = "UPDATE api_test_batch SET total_cases = %s, status = %s WHERE id = %s"
                cursor.execute(update_batch_sql, [0, 2, batch_id])  # status=2è¡¨ç¤ºå®Œæˆ
                conn.commit()
                cursor.close()
                conn.close()
                global_logger.info(f"æ‰¹æ¬¡ {batch_id} çŠ¶æ€æ›´æ–°ä¸ºå®Œæˆï¼Œæµ‹è¯•ç”¨ä¾‹æ•°é‡ä¸º0")
            except Exception as e:
                global_logger.error(f"æ›´æ–°æ‰¹æ¬¡çŠ¶æ€å¤±è´¥: {str(e)}")

                # è¿”å›æˆåŠŸå“åº”ï¼Œä½†è¯´æ˜æ²¡æœ‰ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
            response = format.resp_format_success.copy()
            response["message"] = "æœ¬æ¬¡AIç”Ÿæˆæœªäº§ç”Ÿæœ‰æ•ˆæµ‹è¯•ç”¨ä¾‹ï¼Œè¯·é‡è¯•æˆ–è°ƒæ•´å‚æ•°"
            response["data"] = {
                "total_generated": 0,
                "total_saved": 0,
                "total_failed": 0,
                "testcase_ids": [],
                "batch_id": batch_id,
                "batch_name": batch_name,
                "interface_info": {
                    "interface_id": interface_data['interface_id'],
                    "app_id": interface_data['app_id'],
                    "name": interface_data['name'],
                    "url": interface_data['url'],
                    "method": interface_data['method']
                }
            }
            global_logger.info("=== AIç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å®Œæˆï¼ˆæ— æœ‰æ•ˆç”¨ä¾‹ç”Ÿæˆï¼‰ ===")
            return response

        global_logger.info(f"AIç”Ÿæˆäº† {len(testcases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")

        # æ­¥éª¤3ï¼šé€šè¿‡HTTPè°ƒç”¨ä¿å­˜æµ‹è¯•ç”¨ä¾‹ï¼ˆå¸¦batch_idï¼‰
        global_logger.info("é€šè¿‡HTTPè°ƒç”¨ä¿å­˜æµ‹è¯•ç”¨ä¾‹...")
        saved_testcase_ids = []
        failed_count = 0

        # è·å–å½“å‰æœåŠ¡çš„åŸºç¡€URL
        base_url = request.host_url.rstrip('/')
        add_testcase_url = f"{base_url}/api/testcase/add"

        for i, testcase in enumerate(testcases, 1):
            global_logger.info(f"ä¿å­˜ç¬¬ {i}/{len(testcases)} ä¸ªæµ‹è¯•ç”¨ä¾‹: {testcase.get('name', '')}")

            # æ„å»ºè¯·æ±‚æ•°æ® - æ·»åŠ batch_idå’Œai_generated
            testcase_data = {
                "interface_id": interface_data['interface_id'],
                "app_id": interface_data['app_id'],
                "name": testcase.get('name', f'æµ‹è¯•ç”¨ä¾‹_{i}'),
                "priority": testcase.get('priority', 2),
                "request_url": testcase.get('request_url', interface_data['url']),
                "request_method": testcase.get('request_method', interface_data['method']),
                "request_headers": testcase.get('request_headers', {}),
                "request_params": testcase.get('request_params', {}),
                "expected_status": testcase.get('expected_status', 200),
                "assertions": testcase.get('assertions', [{"type": "status_code", "expected": 200}]),
                "pre_script": testcase.get('pre_script', ''),
                "post_script": testcase.get('post_script', ''),
                "description": testcase.get('description', f'AIç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹ - {testcase.get("name", "")}'),
                "status": testcase.get('status', 1),
                "creator_id": interface_data['creator_id'],
                "creator_name": interface_data['creator_name'],
                "batch_id": batch_id,  # å…³é”®ï¼šè®¾ç½®æ‰¹æ¬¡ID
                "ai_generated": 1  # å…³é”®ï¼šæ ‡è®°ä¸ºAIç”Ÿæˆ
            }

            global_logger.info(f"å‡†å¤‡ä¿å­˜æµ‹è¯•ç”¨ä¾‹æ•°æ®: {testcase_data}")

            try:
                # å‘èµ·HTTPè¯·æ±‚
                response_obj = requests.post(
                    add_testcase_url,
                    json=testcase_data,
                    headers={'Content-Type': 'application/json'},
                    timeout=30
                )

                global_logger.info(f"HTTPè¯·æ±‚çŠ¶æ€ç : {response_obj.status_code}")
                global_logger.info(f"HTTPå“åº”å†…å®¹: {response_obj.text}")

                if response_obj.status_code == 200:
                    result = response_obj.json()
                    if result.get('code') in [200, 20000]:
                        testcase_id = result['data']['id']
                        saved_testcase_ids.append(testcase_id)
                        global_logger.info(f"æµ‹è¯•ç”¨ä¾‹ {i} ä¿å­˜æˆåŠŸï¼ŒID: {testcase_id}")
                    else:
                        failed_count += 1
                        global_logger.error(f"æµ‹è¯•ç”¨ä¾‹ {i} ä¿å­˜å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                else:
                    failed_count += 1
                    global_logger.error(
                        f"æµ‹è¯•ç”¨ä¾‹ {i} HTTPè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response_obj.status_code}, å“åº”: {response_obj.text}")

            except Exception as e:
                failed_count += 1
                global_logger.error(f"æµ‹è¯•ç”¨ä¾‹ {i} ä¿å­˜å¼‚å¸¸: {str(e)}")

                # æ­¥éª¤4ï¼šæ›´æ–°æ‰¹æ¬¡çš„æµ‹è¯•ç”¨ä¾‹æ•°é‡
        try:
            conn = mysql_pool.connection()
            cursor = conn.cursor()

            update_batch_sql = "UPDATE api_test_batch SET total_cases = %s, status = %s WHERE id = %s"
            cursor.execute(update_batch_sql, [len(saved_testcase_ids), 2, batch_id])  # status=2è¡¨ç¤ºå®Œæˆ

            conn.commit()
            cursor.close()
            conn.close()

            global_logger.info(f"æ‰¹æ¬¡ {batch_id} æµ‹è¯•ç”¨ä¾‹æ•°é‡æ›´æ–°ä¸º: {len(saved_testcase_ids)}")

        except Exception as e:
            global_logger.error(f"æ›´æ–°æ‰¹æ¬¡æµ‹è¯•ç”¨ä¾‹æ•°é‡å¤±è´¥: {str(e)}")

            # è¿”å›ç»“æœï¼ˆåŒ…å«batch_idï¼‰
        response = format.resp_format_success.copy()

        # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´æ¶ˆæ¯
        if len(saved_testcase_ids) == 0:
            response["message"] = f"AIç”Ÿæˆäº† {len(testcases)} ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œä½†ä¿å­˜æ—¶å…¨éƒ¨å¤±è´¥"
        elif failed_count == 0:
            response["message"] = f"AIç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å®Œæˆï¼ŒæˆåŠŸä¿å­˜ {len(saved_testcase_ids)} ä¸ª"
        else:
            response["message"] = f"AIç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å®Œæˆï¼ŒæˆåŠŸä¿å­˜ {len(saved_testcase_ids)} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª"

        response["data"] = {
            "total_generated": len(testcases),
            "total_saved": len(saved_testcase_ids),
            "total_failed": failed_count,
            "testcase_ids": saved_testcase_ids,
            "batch_id": batch_id,  # å…³é”®ï¼šè¿”å›æ‰¹æ¬¡ID
            "batch_name": batch_name,  # è¿”å›æ‰¹æ¬¡åç§°
            "interface_info": {
                "interface_id": interface_data['interface_id'],
                "app_id": interface_data['app_id'],
                "name": interface_data['name'],
                "url": interface_data['url'],
                "method": interface_data['method']
            }
        }

        global_logger.info("=== AIç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å®Œæˆ ===")
        return response

    except Exception as e:
        global_logger.error(f"AIç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å¼‚å¸¸: {str(e)}")
        global_logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        response = format.resp_format_failed.copy()
        response["message"] = f"ç³»ç»Ÿå¼‚å¸¸: {str(e)}"
        return response

@ai_route.route('/apis/ai_route/full-test-flow', methods=['POST'])
def full_test_flow():
    """å®Œæ•´çš„AIæµ‹è¯•æµç¨‹ï¼šç”Ÿæˆ->æ‰§è¡Œ->åˆ†æ->æŠ¥å‘Š"""
    global_logger.info("=== å¼€å§‹å®Œæ•´AIæµ‹è¯•æµç¨‹ ===")

    try:
        # è·å–è¯·æ±‚å‚æ•°
        flow_data = request.get_json()
        if not flow_data:
            response = format.resp_format_failed.copy()
            response["message"] = "è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©º"
            return response

            # åŸºç¡€å‚æ•°éªŒè¯
        required_fields = ['app_id', 'name', 'url', 'method']
        missing_fields = [field for field in required_fields if not flow_data.get(field)]
        if missing_fields:
            response = format.resp_format_failed.copy()
            response["message"] = f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(missing_fields)}"
            return response

        interface_id = get_or_create_interface(flow_data)
        flow_data['interface_id'] = interface_id

            # è®¾ç½®é»˜è®¤å€¼
        flow_data.setdefault('interface_id', flow_data.get('app_id') + '_default_if')
        flow_data.setdefault('creator_id', 'ai_system')
        flow_data.setdefault('creator_name', 'AIç³»ç»Ÿ')
        flow_data.setdefault('environment', 'test')
        flow_data.setdefault('batch_name', f"AIæµ‹è¯•-{flow_data['name']}-{datetime.now().strftime('%Y%m%d_%H%M%S')}")

        base_url = request.host_url.rstrip('/')

        # æ­¥éª¤1ï¼šè°ƒç”¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹æ¥å£
        global_logger.info("æ­¥éª¤1: è°ƒç”¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹æ¥å£...")

        generate_url = f"{base_url}/apis/ai_route/generate-testcases"
        generate_response = requests.post(generate_url, json=flow_data, timeout=180)

        if generate_response.status_code != 200:
            response = format.resp_format_failed.copy()
            response["message"] = f"ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹æ¥å£è°ƒç”¨å¤±è´¥: HTTP {generate_response.status_code}"
            return response

        generate_result = generate_response.json()
        if generate_result.get('code') not in [200, 20000]:
            response = format.resp_format_failed.copy()
            response["message"] = f"ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å¤±è´¥: {generate_result.get('message')}"
            return response

        testcase_ids = generate_result['data']['testcase_ids']
        if not testcase_ids:
            response = format.resp_format_failed.copy()
            response["message"] = "æ²¡æœ‰æˆåŠŸç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"
            return response

        global_logger.info(f"æ­¥éª¤1å®Œæˆ: ç”Ÿæˆäº† {len(testcase_ids)} ä¸ªæµ‹è¯•ç”¨ä¾‹")

        # æ­¥éª¤2ï¼šè°ƒç”¨æ‰¹é‡æ‰§è¡Œæ¥å£
        global_logger.info("æ­¥éª¤2: è°ƒç”¨æ‰¹é‡æ‰§è¡Œæ¥å£...")

        batch_execute_url = f"{base_url}/api/testexec/batch_execute"

        batch_data = {
            "name": flow_data['batch_name'],
            "app_id": flow_data['app_id'],
            "testcase_ids": testcase_ids,
            "environment": flow_data['environment'],
            "variables": flow_data.get('variables', {}),
            "test_request_id": flow_data.get('test_request_id', ""),
            "batch_id": generate_result['data']['batch_id']  # ä¼ é€’batch_id
        }

        batch_response = requests.post(batch_execute_url, json=batch_data, timeout=60)

        if batch_response.status_code != 200:
            response = format.resp_format_failed.copy()
            response["message"] = f"æ‰¹é‡æ‰§è¡Œæ¥å£è°ƒç”¨å¤±è´¥: HTTP {batch_response.status_code}"
            return response

        batch_result = batch_response.json()
        if batch_result.get('code') not in [200, 20000]:
            response = format.resp_format_failed.copy()
            response["message"] = f"æ‰¹é‡æ‰§è¡Œå¤±è´¥: {batch_result.get('message')}"
            return response

        batch_id = batch_result['data']['batch_id']
        global_logger.info(f"æ­¥éª¤2å®Œæˆ: æ‰¹é‡æ‰§è¡ŒID = {batch_id}")

        # æ­¥éª¤3ï¼šç­‰å¾…æ‰§è¡Œå®Œæˆå¹¶è·å–ç»“æœ
        global_logger.info("æ­¥éª¤3: ç­‰å¾…æ‰§è¡Œå®Œæˆå¹¶è·å–ç»“æœ...")
        # ç­‰å¾…æ‰¹æ¬¡å®Œæˆ
        batch_completed = wait_for_batch_completion(batch_id)
        if not batch_completed:
            global_logger.warning("æ‰¹æ¬¡å¯èƒ½æœªå®Œå…¨æ‰§è¡Œå®Œæˆï¼Œä½†ç»§ç»­ç”ŸæˆæŠ¥å‘Š")

            # ç„¶åå†è·å–ç»“æœ
        global_logger.info("å¼€å§‹è·å–æµ‹è¯•ç»“æœ...")

        # ç­‰å¾…æ‰¹æ¬¡æ‰§è¡Œå®Œæˆï¼ˆæ”¹è¿›ç‰ˆï¼‰
        global_logger.info(f"ç­‰å¾…æ‰¹æ¬¡ {batch_id} æ‰§è¡Œå®Œæˆ...")

        # å…ˆç­‰å¾…ä¸€æ®µæ—¶é—´è®©æµ‹è¯•å¼€å§‹
        time.sleep(3)

        # å¾ªç¯æ£€æŸ¥ç›´åˆ°æœ‰ç»“æœ
        max_wait_time = 120  # æœ€å¤§ç­‰å¾…2åˆ†é’Ÿ
        wait_interval = 5  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
        waited_time = 0

        test_results = []
        while waited_time < max_wait_time:
            try:
                conn = mysql_pool.connection()
                cursor = conn.cursor()

                # æ£€æŸ¥æ˜¯å¦æœ‰æµ‹è¯•ç»“æœ
                check_sql = "SELECT COUNT(*) FROM api_result WHERE batch_id = %s"
                global_logger.info(f"æ‰§è¡ŒSQL: {check_sql}, å‚æ•°: {batch_id}")
                cursor.execute(check_sql, [batch_id])

                result_row = cursor.fetchone()
                global_logger.info(f"æŸ¥è¯¢ç»“æœ: {result_row}")

                #å¤„ç†å­—å…¸æ ¼å¼çš„è¿”å›ç»“æœ
                if result_row:
                    if isinstance(result_row, dict):
                        result_count = result_row.get('COUNT(*)', 0)  # å­—å…¸æ ¼å¼
                    else:
                        result_count = result_row[0]  # å…ƒç»„æ ¼å¼
                else:
                    result_count = 0
                global_logger.info(f"ç»“æœæ•°é‡: {result_count}")

                global_logger.info(f"ç­‰å¾…ä¸­... å½“å‰ç»“æœæ•°: {result_count}, å·²ç­‰å¾…: {waited_time}s")
                if result_count > 0:
                    global_logger.info(f"æ£€æµ‹åˆ° {result_count} ä¸ªæµ‹è¯•ç»“æœï¼Œå¼€å§‹è·å–è¯¦ç»†æ•°æ®")
                    break

                cursor.close()
                conn.close()

            except Exception as e:
                global_logger.error(f"æ£€æŸ¥æµ‹è¯•ç»“æœæ—¶å‡ºé”™: {str(e)}")
                if 'cursor' in locals():
                    cursor.close()
                if 'conn' in locals():
                    conn.close()

            time.sleep(wait_interval)
            waited_time += wait_interval

        if waited_time >= max_wait_time:
            global_logger.warning(f"ç­‰å¾…è¶…æ—¶({max_wait_time}s)ï¼Œå¯èƒ½æµ‹è¯•è¿˜åœ¨æ‰§è¡Œä¸­")

            # æœ€ç»ˆæŸ¥è¯¢æ‰€æœ‰ç»“æœ
        try:
            conn = mysql_pool.connection()
            cursor = conn.cursor()

            result_sql = """  
                SELECT r.id, r.testcase_id, t.name as testcase_name,   
                       r.request_url, r.request_method, r.response_status,   
                       r.is_success, r.execution_time, r.execute_time,  
                       r.error_message, r.request_body, r.response_body  
                FROM api_result r  
                LEFT JOIN api_testcase t ON r.testcase_id = t.id  
                WHERE r.batch_id = %s  
                ORDER BY r.execute_time  
            """

            cursor.execute(result_sql, [batch_id])
            results = cursor.fetchall()

            global_logger.info(f"æœ€ç»ˆæŸ¥è¯¢åˆ° {len(results)} ä¸ªæµ‹è¯•ç»“æœ")

            # å¤„ç†æŸ¥è¯¢ç»“æœ
            for result in results:
                # å°†æ•°æ®åº“ç»“æœè½¬æ¢ä¸ºå­—å…¸
                if not isinstance(result, dict):
                    columns = ['id', 'testcase_id', 'testcase_name', 'request_url',
                               'request_method', 'response_status', 'is_success',
                               'execution_time', 'execute_time', 'error_message',
                               'request_body', 'response_body']
                    result_dict = dict(zip(columns, result))
                else:
                    result_dict = result

                    # æ·»åŠ statuså­—æ®µ
                result_dict['status'] = 'PASS' if result_dict.get('is_success') else 'FAIL'

                # è§£æJSONå­—æ®µ
                try:
                    if 'request_body' in result_dict and result_dict['request_body']:
                        if isinstance(result_dict['request_body'], str):
                            result_dict['request_body'] = json.loads(result_dict['request_body'])
                    if 'response_body' in result_dict and result_dict['response_body']:
                        if isinstance(result_dict['response_body'], str):
                            result_dict['response_body'] = json.loads(result_dict['response_body'])
                except Exception as e:
                    global_logger.warning(f"è§£æJSONå­—æ®µå¤±è´¥: {str(e)}")

                test_results.append(result_dict)

        except Exception as e:
            global_logger.error(f"æŸ¥è¯¢æµ‹è¯•ç»“æœå‡ºé”™: {str(e)}")
            global_logger.error(traceback.format_exc())
        finally:
            cursor.close()
            conn.close()

        global_logger.info(f"æ­¥éª¤3å®Œæˆ: è·å–åˆ° {len(test_results)} ä¸ªæµ‹è¯•ç»“æœ")

        # æ­¥éª¤4-5ï¼šå¹¶è¡Œå¤„ç†ExcelæŠ¥å‘Šå’Œé‚®ä»¶å‘é€
        global_logger.info("æ­¥éª¤4-5: å¹¶è¡Œç”ŸæˆExcelæŠ¥å‘Šå’Œå‘é€é‚®ä»¶...")

        # è®¡ç®—æµ‹è¯•ç»“æœç»Ÿè®¡ä¿¡æ¯ï¼ˆæå‰è®¡ç®—ï¼Œä¾›ä¸¤ä¸ªä»»åŠ¡ä½¿ç”¨ï¼‰
        total_cases = len(test_results)
        success_count_by_is_success = sum(1 for result in test_results if result.get('is_success') == True)
        success_count_by_response_status = sum(
            1 for result in test_results if 200 <= result.get('response_status', 0) < 300)

        # é€‰æ‹©æœ€åˆé€‚çš„æˆåŠŸæ•°é‡
        if success_count_by_is_success > 0:
            success_count = success_count_by_is_success
        else:
            success_count = success_count_by_response_status

        failed_count = total_cases - success_count
        success_rate = (success_count / total_cases * 100) if total_cases > 0 else 0

        global_logger.info(
            f"æµ‹è¯•ç»Ÿè®¡: æ€»æ•°={total_cases}, æˆåŠŸ={success_count}, å¤±è´¥={failed_count}, æˆåŠŸç‡={success_rate:.1f}%")

        # å‡†å¤‡å…±äº«æ•°æ®
        shared_data = {
            'batch_id': batch_id,
            'test_results': test_results,
            'total_cases': total_cases,
            'success_count': success_count,
            'failed_count': failed_count,
            'success_rate': success_rate,
            'flow_data': flow_data,
            'base_url': base_url
        }

        # å¹¶è¡Œå¤„ç†Excelå’Œé‚®ä»¶
        report_url = None
        email_task_id = None

        def generate_excel_task(shared_data):
            """ç”ŸæˆExcelæŠ¥å‘Šä»»åŠ¡"""
            try:
                global_logger.info("å¼€å§‹ç”ŸæˆExcelæŠ¥å‘Š...")
                export_url = f"{shared_data['base_url']}/api/testexec/export_report?batch_id={shared_data['batch_id']}"
                global_logger.info(f"ExcelæŠ¥å‘Šç”Ÿæˆå®Œæˆ: {export_url}")
                return export_url
            except Exception as e:
                global_logger.error(f"ç”ŸæˆExcelæŠ¥å‘Šå¤±è´¥: {str(e)}")
                return None

        def send_email_task(shared_data):
            """å‘é€é‚®ä»¶ä»»åŠ¡ - AIå¢å¼ºç‰ˆ"""
            try:
                global_logger.info("å¼€å§‹å‘é€é‚®ä»¶æŠ¥å‘Š...")

                # ğŸ”§ é‡æ–°æŸ¥è¯¢æ•°æ®åº“è·å–æµ‹è¯•ç»“æœ
                test_results = []
                try:
                    conn = mysql_pool.connection()
                    cursor = conn.cursor()

                    result_sql = """  
                        SELECT r.id, r.testcase_id, t.name as testcase_name,   
                               r.request_url, r.request_method, r.response_status,   
                               r.is_success, r.execution_time, r.execute_time,  
                               r.error_message, r.request_body, r.response_body  
                        FROM api_result r  
                        LEFT JOIN api_testcase t ON r.testcase_id = t.id  
                        WHERE r.batch_id = %s  
                        ORDER BY r.execute_time  
                    """

                    cursor.execute(result_sql, [shared_data['batch_id']])
                    results = cursor.fetchall()

                    global_logger.info(f"é‚®ä»¶ä»»åŠ¡é‡æ–°æŸ¥è¯¢åˆ° {len(results)} æ¡ç»“æœ")

                    # å¤„ç†æŸ¥è¯¢ç»“æœ
                    for result in results:
                        if not isinstance(result, dict):
                            columns = ['id', 'testcase_id', 'testcase_name', 'request_url',
                                       'request_method', 'response_status', 'is_success',
                                       'execution_time', 'execute_time', 'error_message',
                                       'request_body', 'response_body']
                            result_dict = dict(zip(columns, result))
                        else:
                            result_dict = result

                        result_dict['status'] = 'PASS' if result_dict.get('is_success') else 'FAIL'
                        test_results.append(result_dict)

                except Exception as e:
                    global_logger.error(f"é‚®ä»¶ä»»åŠ¡æŸ¥è¯¢æ•°æ®åº“å¤±è´¥: {str(e)}")
                    test_results = []
                finally:
                    cursor.close()
                    conn.close()

                    # é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                total_cases = len(test_results)
                success_count = sum(1 for result in test_results if result.get('is_success') == True)
                failed_count = total_cases - success_count
                success_rate = (success_count / total_cases * 100) if total_cases > 0 else 0

                global_logger.info(f"é‚®ä»¶ä»»åŠ¡ç»Ÿè®¡: æ€»æ•°={total_cases}, æˆåŠŸ={success_count}, å¤±è´¥={failed_count}")

                # ç¡®å®šæ•´ä½“çŠ¶æ€å’Œé£é™©çº§åˆ«
                overall_status = 'PASS' if failed_count == 0 else 'FAIL'
                risk_level = 'LOW' if success_rate >= 90 else ('MEDIUM' if success_rate >= 70 else 'HIGH')

                # ğŸ”¥ AIæ™ºèƒ½åˆ†æ
                ai_analysis = None
                analysis_type = 'RULE_BASED'

                try:
                    if failed_count > 0:  # åªæœ‰å­˜åœ¨å¤±è´¥æ—¶æ‰è¿›è¡ŒAIåˆ†æ
                        global_logger.info("å¼€å§‹AIæ™ºèƒ½åˆ†æå¤±è´¥åŸå› ...")
                        ai_analysis = _ai_analyze_failures(test_results, shared_data['flow_data'])
                        if ai_analysis:
                            analysis_type = 'AI_ENHANCED'
                            global_logger.info("AIåˆ†æå®Œæˆ")
                        else:
                            global_logger.warning("AIåˆ†æè¿”å›ç©ºç»“æœï¼Œä½¿ç”¨è§„åˆ™åˆ†æ")
                    else:
                        global_logger.info("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œè·³è¿‡AIåˆ†æ")
                except Exception as e:
                    global_logger.error(f"AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åˆ†æ: {str(e)}")

                    # å‡†å¤‡åˆ†ææ•°æ®
                analysis_data = {
                    'summary': {
                        'total_cases': total_cases,
                        'success_count': success_count,
                        'failed_count': failed_count,
                        'success_rate': success_rate,
                        'overall_status': overall_status,
                        'risk_level': risk_level,
                        'execution_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'analysis_type': analysis_type
                    },
                    'key_findings': [
                        f"æ¥å£ {shared_data['flow_data']['name']} æµ‹è¯•å®Œæˆï¼Œå…±æ‰§è¡Œ {total_cases} ä¸ªæµ‹è¯•ç”¨ä¾‹",
                        f"æˆåŠŸç‡: {success_rate:.1f}%ï¼Œé€šè¿‡ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª"
                    ],
                    'failure_analysis': [],
                    'recommendations': [],
                    'next_steps': [
                        "æŸ¥çœ‹è¯¦ç»†æµ‹è¯•æŠ¥å‘Šä»¥äº†è§£æ›´å¤šä¿¡æ¯",
                        f"è®¿é—®æŠ¥å‘Šé“¾æ¥: {shared_data['base_url']}/api/testexec/export_report?batch_id={shared_data['batch_id']}"
                    ]
                }

                # ğŸ”¥ é›†æˆAIåˆ†æç»“æœ
                if ai_analysis and analysis_type == 'AI_ENHANCED':
                    global_logger.info("é›†æˆAIåˆ†æç»“æœåˆ°é‚®ä»¶æŠ¥å‘Š")

                    # æ·»åŠ AIåˆ†ææ ‡è¯†
                    analysis_data['ai_insights'] = ai_analysis
                    analysis_data['analysis_powered_by'] = 'AI + è§„åˆ™åˆ†æ'

                    # ä½¿ç”¨AIç”Ÿæˆçš„å†…å®¹
                    if ai_analysis.get('failure_analysis'):
                        analysis_data['failure_analysis'] = ai_analysis['failure_analysis']

                    if ai_analysis.get('recommendations'):
                        analysis_data['recommendations'] = ai_analysis['recommendations']

                    if ai_analysis.get('key_findings'):
                        analysis_data['key_findings'].extend(ai_analysis['key_findings'])

                        # æ·»åŠ AIä¸“ä¸šæ´å¯Ÿ
                    if ai_analysis.get('root_cause'):
                        analysis_data['root_cause_analysis'] = ai_analysis['root_cause']

                    if ai_analysis.get('risk_assessment'):
                        analysis_data['ai_risk_assessment'] = ai_analysis['risk_assessment']

                else:
                    # ä½¿ç”¨åŸæœ‰çš„è§„åˆ™åˆ†æ
                    analysis_data['analysis_powered_by'] = 'è§„åˆ™åˆ†æ'

                    if failed_count > 0:
                        failure_reasons = []
                        for result in test_results:
                            if not result.get('is_success'):
                                reason = result.get('error_message', 'æœªçŸ¥é”™è¯¯')
                                testcase_name = result.get('testcase_name',
                                                           f"ç”¨ä¾‹ID: {result.get('testcase_id', 'unknown')}")
                                failure_reasons.append(f"{testcase_name}: {reason}")

                        analysis_data['failure_analysis'] = failure_reasons
                        analysis_data['recommendations'] = [
                            "æ£€æŸ¥æ¥å£å‚æ•°éªŒè¯é€»è¾‘",
                            "ç¡®è®¤æ¥å£è¿”å›å€¼æ˜¯å¦ç¬¦åˆé¢„æœŸ",
                            "éªŒè¯è¾¹ç•Œæ¡ä»¶å’Œå¼‚å¸¸åœºæ™¯å¤„ç†"
                        ]
                    else:
                        analysis_data['recommendations'] = [
                            "ç»§ç»­ä¿æŒè‰¯å¥½çš„æ¥å£è´¨é‡",
                            "è€ƒè™‘æ·»åŠ æ›´å¤šè¾¹ç•Œæ¡ä»¶æµ‹è¯•"
                        ]

                        # æ‰¹æ¬¡ä¿¡æ¯
                batch_info = {
                    'name': shared_data['flow_data']['batch_name'],
                    'id': shared_data['batch_id'],
                    'interface_name': shared_data['flow_data']['name'],
                    'app_id': shared_data['flow_data']['app_id']
                }

                # è·å–æ”¶ä»¶äººåˆ—è¡¨
                recipients = shared_data['flow_data'].get('email_recipients', [])
                if not recipients and hasattr(config, 'DEFAULT_EMAIL_RECIPIENTS'):
                    recipients = config.DEFAULT_EMAIL_RECIPIENTS

                    # å‘é€é‚®ä»¶
                if recipients:
                    from services.notification_service import get_notification_service
                    notification_service = get_notification_service()

                    # å‘é€æµ‹è¯•é‚®ä»¶
                    try:
                        test_recipient = "é‚®ç®±"
                        global_logger.info(f"å‘é€æµ‹è¯•é‚®ä»¶åˆ° {test_recipient}...")
                        test_result = notification_service.send_test_email_directly(test_recipient)
                        global_logger.info(f"æµ‹è¯•é‚®ä»¶å‘é€ç»“æœ: {test_result}")
                    except Exception as e:
                        global_logger.error(f"å‘é€æµ‹è¯•é‚®ä»¶å¼‚å¸¸: {str(e)}")

                        # å‘é€AIå¢å¼ºçš„åˆ†ææŠ¥å‘Š
                    task_id = notification_service.send_analysis_report(
                        analysis_data=analysis_data,
                        batch_info=batch_info,
                        recipients=recipients
                    )

                    global_logger.info(f"AIå¢å¼ºé‚®ä»¶å‘é€ä»»åŠ¡å·²æäº¤ï¼Œä»»åŠ¡ID: {task_id}")
                    return task_id
                else:
                    global_logger.info("æœªæŒ‡å®šæ”¶ä»¶äººï¼Œè·³è¿‡é‚®ä»¶å‘é€")
                    return None

            except Exception as e:
                global_logger.error(f"å‘é€é‚®ä»¶ä»»åŠ¡å¤±è´¥: {str(e)}")
                global_logger.error(traceback.format_exc())
                return None

        def _ai_analyze_failures(test_results, flow_data):
            """AIåˆ†æå¤±è´¥åŸå› """
            try:
                failed_cases = [r for r in test_results if not r.get('is_success')]

                if not failed_cases:
                    return None

                global_logger.info(f"å¼€å§‹AIåˆ†æ {len(failed_cases)} ä¸ªå¤±è´¥ç”¨ä¾‹")

                # æ„å»ºå¤±è´¥ç”¨ä¾‹è¯¦æƒ…
                failure_details = []
                for case in failed_cases[:10]:  # åˆ†æå‰10ä¸ªå¤±è´¥æ¡ˆä¾‹
                    failure_details.append({
                        'testcase_name': case.get('testcase_name', 'Unknown'),
                        'error_message': case.get('error_message', ''),
                        'status_code': case.get('response_status', ''),
                        'request_method': case.get('request_method', ''),
                        'request_url': case.get('request_url', ''),
                        'request_body': case.get('request_body', '')[:500] if case.get('request_body') else '',
                        # é™åˆ¶é•¿åº¦
                        'response_body': case.get('response_body', '')[:500] if case.get('response_body') else ''
                    })

                    # è®¡ç®—å¤±è´¥ç»Ÿè®¡
                total_failures = len(failed_cases)
                error_codes = {}
                error_messages = {}

                for case in failed_cases:
                    # ç»Ÿè®¡çŠ¶æ€ç 
                    status_code = case.get('response_status', 'unknown')
                    error_codes[status_code] = error_codes.get(status_code, 0) + 1

                    # ç»Ÿè®¡é”™è¯¯æ¶ˆæ¯
                    error_msg = case.get('error_message', 'unknown')
                    error_messages[error_msg] = error_messages.get(error_msg, 0) + 1

                    # æ„å»ºAIåˆ†ææç¤º
                prompt = f"""  
        ä½œä¸ºèµ„æ·±æ¥å£æµ‹è¯•ä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹æµ‹è¯•å¤±è´¥æƒ…å†µï¼š  

        æ¥å£åŸºæœ¬ä¿¡æ¯ï¼š  
        - æ¥å£åç§°ï¼š{flow_data['name']}  
        - æ¥å£åœ°å€ï¼š{flow_data['url']}  
        - è¯·æ±‚æ–¹æ³•ï¼š{flow_data['method']}  
        - æ€»å¤±è´¥æ•°ï¼š{total_failures}  

        å¤±è´¥ç»Ÿè®¡ï¼š  
        çŠ¶æ€ç åˆ†å¸ƒï¼š{dict(list(error_codes.items())[:5])}  
        é”™è¯¯æ¶ˆæ¯åˆ†å¸ƒï¼š{dict(list(error_messages.items())[:5])}  

        å…¸å‹å¤±è´¥æ¡ˆä¾‹ï¼š  
        {json.dumps(failure_details[:5], ensure_ascii=False, indent=2)}  

        è¯·ä»ä¸“ä¸šè§’åº¦åˆ†æå¹¶æä¾›JSONæ ¼å¼å›ç­”ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š  
        1. "failure_analysis": [å¤±è´¥åŸå› åˆ†æåˆ—è¡¨]  
        2. "key_findings": [å…³é”®å‘ç°åˆ—è¡¨]  
        3. "recommendations": [æ”¹è¿›å»ºè®®åˆ—è¡¨]  
        4. "root_cause": "æ ¹æœ¬åŸå› åˆ†æ"  
        5. "risk_assessment": "é£é™©è¯„ä¼°"  

        è¦æ±‚ï¼š  
        - åˆ†æè¦ä¸“ä¸šã€å…·ä½“ã€æœ‰é’ˆå¯¹æ€§  
        - å»ºè®®è¦å¯æ‰§è¡Œã€æœ‰ä¼˜å…ˆçº§  
        - è¯­è¨€ç®€æ´æ˜äº†ï¼Œæ¯æ¡ä¸è¶…è¿‡50å­—  
        - å¿…é¡»è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼  
        """

                # è°ƒç”¨AIæœåŠ¡
                from services.ai_service import get_ai_service
                ai_service = get_ai_service()

                global_logger.info("æ­£åœ¨è°ƒç”¨AIæœåŠ¡è¿›è¡Œåˆ†æ...")
                response = ai_service.generate_response(prompt)

                if not response:
                    global_logger.warning("AIæœåŠ¡è¿”å›ç©ºå“åº”")
                    return None

                global_logger.info(f"AIæœåŠ¡è¿”å›å“åº”é•¿åº¦: {len(response)}")

                # è§£æAIå“åº”
                try:
                    import json
                    import re

                    # å°è¯•æå–JSONéƒ¨åˆ†
                    json_match = re.search(r'\{.*\}', response, re.DOTALL)
                    if json_match:
                        json_str = json_match.group()
                        ai_result = json.loads(json_str)

                        # éªŒè¯å¿…è¦å­—æ®µ
                        required_fields = ['failure_analysis', 'key_findings', 'recommendations']
                        for field in required_fields:
                            if field not in ai_result:
                                ai_result[field] = []

                        global_logger.info("AIåˆ†æç»“æœè§£ææˆåŠŸ")
                        return ai_result

                    else:
                        global_logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆJSONæ ¼å¼ï¼Œä½¿ç”¨æ–‡æœ¬è§£æ")
                        return _parse_text_response(response)

                except json.JSONDecodeError as e:
                    global_logger.error(f"JSONè§£æå¤±è´¥: {str(e)}")
                    return _parse_text_response(response)

            except Exception as e:
                global_logger.error(f"AIåˆ†æè°ƒç”¨å¤±è´¥: {str(e)}")
                global_logger.error(traceback.format_exc())
                return None

        def _parse_text_response(response):
            """è§£æAIæ–‡æœ¬å“åº”ä¸ºç»“æ„åŒ–æ•°æ®"""
            try:
                # ç®€å•çš„æ–‡æœ¬è§£æé€»è¾‘
                lines = response.split('\n')

                result = {
                    'failure_analysis': [],
                    'key_findings': [],
                    'recommendations': [],
                    'root_cause': '',
                    'risk_assessment': ''
                }

                current_section = None

                for line in lines:
                    line = line.strip()
                    if not line:
                        continue

                        # è¯†åˆ«ç« èŠ‚
                    if 'å¤±è´¥åŸå› ' in line or 'failure_analysis' in line or 'åŸå› åˆ†æ' in line:
                        current_section = 'failure_analysis'
                    elif 'å…³é”®å‘ç°' in line or 'key_findings' in line or 'ä¸»è¦å‘ç°' in line:
                        current_section = 'key_findings'
                    elif 'å»ºè®®' in line or 'recommendations' in line or 'æ”¹è¿›å»ºè®®' in line:
                        current_section = 'recommendations'
                    elif 'æ ¹æœ¬åŸå› ' in line or 'root_cause' in line or 'æ ¹å› ' in line:
                        current_section = 'root_cause'
                    elif 'é£é™©è¯„ä¼°' in line or 'risk_assessment' in line or 'é£é™©åˆ†æ' in line:
                        current_section = 'risk_assessment'
                    elif line.startswith('-') or line.startswith('â€¢') or line.startswith('*') or line.startswith(
                            '1.') or line.startswith('2.'):
                        # åˆ—è¡¨é¡¹
                        item = line.lstrip('-â€¢*123456789. ')
                        if current_section in ['failure_analysis', 'key_findings', 'recommendations'] and item:
                            result[current_section].append(item)
                    elif current_section in ['root_cause', 'risk_assessment'] and line:
                        # å•è¡Œæ–‡æœ¬
                        if result[current_section]:
                            result[current_section] += ' ' + line
                        else:
                            result[current_section] = line

                            # å¦‚æœè§£æç»“æœä¸ºç©ºï¼Œæ·»åŠ é»˜è®¤å†…å®¹
                if not any(result[key] for key in ['failure_analysis', 'key_findings', 'recommendations']):
                    result['failure_analysis'] = ['AIåˆ†æäº†æµ‹è¯•å¤±è´¥æƒ…å†µ']
                    result['key_findings'] = ['å‘ç°å¤šä¸ªæµ‹è¯•ç”¨ä¾‹æ‰§è¡Œå¤±è´¥']
                    result['recommendations'] = ['å»ºè®®æ£€æŸ¥æ¥å£å®ç°å’Œæµ‹è¯•æ•°æ®']
                    result['root_cause'] = 'éœ€è¦è¿›ä¸€æ­¥åˆ†æå¤±è´¥åŸå› '
                    result['risk_assessment'] = 'å­˜åœ¨æ¥å£è´¨é‡é£é™©'

                global_logger.info("AIæ–‡æœ¬å“åº”è§£æå®Œæˆ")
                return result

            except Exception as e:
                global_logger.error(f"æ–‡æœ¬è§£æå¤±è´¥: {str(e)}")
                # è¿”å›åŸºç¡€åˆ†æç»“æœ
                return {
                    'failure_analysis': ['AIåˆ†æé‡åˆ°é—®é¢˜ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æ—¥å¿—'],
                    'key_findings': ['æµ‹è¯•æ‰§è¡Œå­˜åœ¨å¤±è´¥æƒ…å†µ'],
                    'recommendations': ['å»ºè®®äººå·¥æ£€æŸ¥å¤±è´¥åŸå› '],
                    'root_cause': 'åˆ†æè¿‡ç¨‹ä¸­é‡åˆ°æŠ€æœ¯é—®é¢˜',
                    'risk_assessment': 'éœ€è¦äººå·¥è¯„ä¼°é£é™©'
                }

        def _validate_ai_analysis(ai_analysis):
            """éªŒè¯AIåˆ†æç»“æœçš„å®Œæ•´æ€§"""
            try:
                if not ai_analysis or not isinstance(ai_analysis, dict):
                    return False

                    # æ£€æŸ¥å¿…éœ€å­—æ®µ
                required_fields = ['failure_analysis', 'key_findings', 'recommendations']
                for field in required_fields:
                    if field not in ai_analysis:
                        return False
                    if not isinstance(ai_analysis[field], list):
                        return False

                        # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…å†…å®¹
                has_content = any(
                    len(ai_analysis[field]) > 0
                    for field in required_fields
                )

                return has_content

            except Exception as e:
                global_logger.error(f"AIåˆ†æç»“æœéªŒè¯å¤±è´¥: {str(e)}")
                return False

        def _get_fallback_analysis(test_results, flow_data):
            """è·å–é™çº§åˆ†æç»“æœ"""
            try:
                failed_cases = [r for r in test_results if not r.get('is_success')]

                # ç»Ÿè®¡é”™è¯¯ç±»å‹
                error_types = {}
                for case in failed_cases:
                    error_msg = case.get('error_message', 'æœªçŸ¥é”™è¯¯')
                    status_code = case.get('response_status', 'unknown')
                    error_key = f"{status_code}: {error_msg}"
                    error_types[error_key] = error_types.get(error_key, 0) + 1

                    # æ„å»ºé™çº§åˆ†æ
                fallback_analysis = {
                    'failure_analysis': [
                        f"æ£€æµ‹åˆ° {len(failed_cases)} ä¸ªå¤±è´¥ç”¨ä¾‹",
                        f"ä¸»è¦é”™è¯¯ç±»å‹: {', '.join(list(error_types.keys())[:3])}"
                    ],
                    'key_findings': [
                        f"å¤±è´¥ç‡: {len(failed_cases) / len(test_results) * 100:.1f}%",
                        f"æ¶‰åŠæ¥å£: {flow_data['name']}"
                    ],
                    'recommendations': [
                        "æ£€æŸ¥æ¥å£å‚æ•°éªŒè¯é€»è¾‘",
                        "ç¡®è®¤æ¥å£è¿”å›å€¼æ˜¯å¦ç¬¦åˆé¢„æœŸ",
                        "éªŒè¯è¾¹ç•Œæ¡ä»¶å’Œå¼‚å¸¸åœºæ™¯å¤„ç†"
                    ],
                    'root_cause': 'éœ€è¦è¿›ä¸€æ­¥åˆ†æå…·ä½“å¤±è´¥åŸå› ',
                    'risk_assessment': 'å­˜åœ¨æ¥å£è´¨é‡é£é™©ï¼Œå»ºè®®åŠæ—¶ä¿®å¤'
                }

                return fallback_analysis

            except Exception as e:
                global_logger.error(f"é™çº§åˆ†æå¤±è´¥: {str(e)}")
                return {
                    'failure_analysis': ['åˆ†æè¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜'],
                    'key_findings': ['å­˜åœ¨æµ‹è¯•å¤±è´¥æƒ…å†µ'],
                    'recommendations': ['å»ºè®®äººå·¥æ£€æŸ¥'],
                    'root_cause': 'åˆ†æå¼‚å¸¸',
                    'risk_assessment': 'éœ€è¦äººå·¥è¯„ä¼°'}


                # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ

        with ThreadPoolExecutor(max_workers=2) as executor:
            # æäº¤ä»»åŠ¡
            excel_future = executor.submit(generate_excel_task, shared_data)
            email_future = executor.submit(send_email_task, shared_data)

            # è·å–ç»“æœ
            try:
                report_url = excel_future.result(timeout=30)
                global_logger.info(f"Excelä»»åŠ¡å®Œæˆ: {report_url}")
            except Exception as e:
                global_logger.error(f"Excelä»»åŠ¡å¤±è´¥: {str(e)}")

            try:
                email_task_id = email_future.result(timeout=30)
                global_logger.info(f"é‚®ä»¶ä»»åŠ¡å®Œæˆ: {email_task_id}")
            except Exception as e:
                global_logger.error(f"é‚®ä»¶ä»»åŠ¡å¤±è´¥: {str(e)}")

        global_logger.info("æ­¥éª¤4-5å®Œæˆ: ExcelæŠ¥å‘Šå’Œé‚®ä»¶å‘é€å¹¶è¡Œå¤„ç†å®Œæˆ")

        # å‡†å¤‡è¿”å›ç»“æœä¸­çš„æ”¶ä»¶äººä¿¡æ¯
        email_recipients = shared_data['flow_data'].get('email_recipients', [])
        if not email_recipients and hasattr(config, 'DEFAULT_EMAIL_RECIPIENTS'):
            email_recipients = config.DEFAULT_EMAIL_RECIPIENTS

            # è¿”å›å®Œæ•´ç»“æœ
        response = format.resp_format_success.copy()
        response["message"] = "å®Œæ•´AIæµ‹è¯•æµç¨‹æ‰§è¡ŒæˆåŠŸ"
        response["data"] = {
            "flow_summary": {
                "interface_name": flow_data['name'],
                "batch_id": batch_id,
                "batch_name": flow_data['batch_name'],
                "testcases_generated": len(testcase_ids),
                "testcases_executed": len(testcase_ids),
                "report_url": report_url
            },
            "generation_result": {
                "testcase_ids": testcase_ids,
                "total_generated": generate_result['data']['total_generated'],
                "total_saved": generate_result['data']['total_saved']
            },
            "execution_result": {
                "batch_id": batch_id,
                "total_cases": batch_result['data']['total_cases']
            },
            "test_results": test_results,
            "report_url": report_url,
            "email_notification": {
                "sent": email_task_id is not None,
                "recipients": email_recipients,  #ä½¿ç”¨æ­£ç¡®çš„å˜é‡
                "task_id": email_task_id
            }
        }

        global_logger.info("=== å®Œæ•´AIæµ‹è¯•æµç¨‹æ‰§è¡Œå®Œæˆ ===")
        return response

    except Exception as e:
        global_logger.error(f"å®Œæ•´AIæµ‹è¯•æµç¨‹å¼‚å¸¸: {str(e)}")
        global_logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        response = format.resp_format_failed.copy()
        response["message"] = f"ç³»ç»Ÿå¼‚å¸¸: {str(e)}"
        return response


def get_test_results(batch_id):
    """è·å–æµ‹è¯•ç»“æœï¼Œç›´æ¥ä»æ•°æ®åº“è·å–"""
    global_logger.info(f"è·å–æµ‹è¯•æ‰¹æ¬¡ {batch_id} çš„æµ‹è¯•ç»“æœ...")

    conn = mysql_pool.connection()
    cursor = conn.cursor()

    try:
        # æŸ¥è¯¢æµ‹è¯•ç»“æœ
        result_sql = """  
            SELECT r.id, r.testcase_id, t.name as testcase_name,   
                   r.request_url, r.request_method, r.response_status,   
                   r.is_success, r.execution_time, r.execute_time,  
                   r.error_message, r.request_headers, r.request_body,  
                   r.response_headers, r.response_body  
            FROM api_result r  
            LEFT JOIN api_testcase t ON r.testcase_id = t.id  
            WHERE r.batch_id = %s  
            ORDER BY r.execute_time  
        """
        global_logger.info(f'æ‰§è¡ŒSQL: {result_sql}, å‚æ•°: [{batch_id}]')
        cursor.execute(result_sql, [batch_id])
        results = cursor.fetchall()

        # è½¬æ¢ç»“æœæ ¼å¼ä»¥åŒ¹é…APIè¿”å›çš„æ ¼å¼
        test_results = []
        for result in results:
            # å°†æ•°æ®åº“ç»“æœè½¬æ¢ä¸ºä¸APIè¿”å›ç›¸åŒçš„æ ¼å¼
            result_item = {
                'id': result['id'],
                'testcase_id': result['testcase_id'],
                'testcase_name': result['testcase_name'],
                'request_url': result['request_url'],
                'request_method': result['request_method'],
                'response_status': result['response_status'],
                'execution_time': result['execution_time'],
                'execute_time': result['execute_time'],
                'error_message': result['error_message'],
                # æ·»åŠ ä¸APIè¿”å›æ ¼å¼åŒ¹é…çš„å­—æ®µ
                'status': 'PASS' if result['is_success'] else 'FAIL',
            }

            # å¤„ç†JSONå­—æ®µ
            try:
                if result['request_headers']:
                    result_item['request_headers'] = json.loads(result['request_headers'])
                if result['request_body']:
                    result_item['request_body'] = json.loads(result['request_body'])
                if result['response_headers']:
                    result_item['response_headers'] = json.loads(result['response_headers'])
                if result['response_body']:
                    result_item['response_body'] = json.loads(result['response_body'])
            except:
                global_logger.warning(f"è§£æJSONå­—æ®µå¤±è´¥ï¼Œæµ‹è¯•ç»“æœID: {result['id']}")

            test_results.append(result_item)

        global_logger.info(f"è·å–åˆ° {len(test_results)} ä¸ªæµ‹è¯•ç»“æœ")

        return test_results
    except Exception as e:
        global_logger.error(f"è·å–æµ‹è¯•ç»“æœå¼‚å¸¸: {str(e)}")
        global_logger.error(traceback.format_exc())
        return []
    finally:
        cursor.close()
        conn.close()

def is_batch_completed(batch_id):
    """æ£€æŸ¥æ‰¹æ¬¡æ˜¯å¦å·²ç»å®Œæˆ"""
    conn = mysql_pool.connection()
    cursor = conn.cursor()
    try:
        sql = "SELECT status FROM api_test_batch WHERE id = %s"
        cursor.execute(sql, [batch_id])
        result = cursor.fetchone()

        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        global_logger.info(f"æ‰¹æ¬¡ {batch_id} æŸ¥è¯¢ç»“æœ: {result}")
        global_logger.info(f"ç»“æœç±»å‹: {type(result)}")

        if result:
            # æ ¹æ®ä¸åŒçš„è¿”å›æ ¼å¼å¤„ç†
            if isinstance(result, dict):
                status = result.get('status')
            elif isinstance(result, tuple):
                status = result[0]
            else:
                status = result

            global_logger.info(f"æ‰¹æ¬¡ {batch_id} çŠ¶æ€: {status}")
            return status == 2
        else:
            global_logger.warning(f"æœªæ‰¾åˆ°æ‰¹æ¬¡ {batch_id}")
            return False

    except Exception as e:
        global_logger.error(f"æ£€æŸ¥æ‰¹æ¬¡çŠ¶æ€å‡ºé”™: {str(e)}")
        return False
    finally:
        cursor.close()
        conn.close()
def wait_for_batch_completion(batch_id):
    """ç­‰å¾…æ‰¹æ¬¡æ‰§è¡Œå®Œæˆ"""
    max_wait_time = 300
    wait_interval = 3
    waited_time = 0

    while waited_time < max_wait_time:
        try:
            conn = mysql_pool.connection()
            cursor = conn.cursor()

            # æ£€æŸ¥æ‰¹æ¬¡çŠ¶æ€
            check_sql = """  
            SELECT status, total_cases, passed_cases, failed_cases   
            FROM api_test_batch   
            WHERE id = %s  
            """
            cursor.execute(check_sql, [batch_id])
            batch_info = cursor.fetchone()

            if batch_info:
                status = batch_info.get('status', '')
                total = batch_info.get('total_cases', 0)
                passed = batch_info.get('passed_cases', 0)
                failed = batch_info.get('failed_cases', 0)
                completed = passed + failed

                global_logger.info(f"æ‰¹æ¬¡çŠ¶æ€: {status}, è¿›åº¦: {completed}/{total}, å·²ç­‰å¾…: {waited_time}s")

                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if status == 'completed' or completed >= total:
                    global_logger.info(f"æ‰¹æ¬¡æ‰§è¡Œå®Œæˆ! çŠ¶æ€: {status}, é€šè¿‡: {passed}, å¤±è´¥: {failed}")
                    cursor.close()
                    conn.close()
                    return True

            cursor.close()
            conn.close()

        except Exception as e:
            global_logger.error(f"æ£€æŸ¥æ‰¹æ¬¡çŠ¶æ€æ—¶å‡ºé”™: {str(e)}")

        time.sleep(wait_interval)
        waited_time += wait_interval

    global_logger.warning(f"ç­‰å¾…æ‰¹æ¬¡å®Œæˆè¶…æ—¶({max_wait_time}s)")
    return False


def get_or_create_interface(flow_data):
    """è·å–æˆ–åˆ›å»ºæ¥å£è®°å½•"""
    try:
        # æŸ¥è¯¢ç°æœ‰æ¥å£ - ä½¿ç”¨ fetch_one
        query = """  
            SELECT id FROM api_interface   
            WHERE app_id = %s AND url = %s AND method = %s  
        """
        result = fetch_one(query, (
            flow_data['app_id'],
            flow_data['url'],
            flow_data['method']
        ))

        # å¦‚æœæ‰¾åˆ°ç°æœ‰æ¥å£ï¼Œè¿”å›ID
        if result:
            global_logger.info(f"æ‰¾åˆ°ç°æœ‰æ¥å£: {result['id']}")
            return result['id']

            # åˆ›å»ºæ–°æ¥å£
        global_logger.info("åˆ›å»ºæ–°æ¥å£è®°å½•...")
        interface_id = generate_id()
        insert_query = """  
            INSERT INTO api_interface (id, app_id, name, url, method, headers, params, create_time)  
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)  
        """
        execute_query(insert_query, (
            interface_id,
            flow_data['app_id'],
            flow_data['name'],
            flow_data['url'],
            flow_data['method'],
            json.dumps(flow_data.get('headers', {})),
            json.dumps(flow_data.get('params', {})),
            int(time.time() * 1000)
        ))

        global_logger.info(f"æ–°æ¥å£åˆ›å»ºæˆåŠŸ: {interface_id}")
        return interface_id

    except Exception as e:
        global_logger.error(f"è·å–æˆ–åˆ›å»ºæ¥å£å¤±è´¥: {str(e)}")
        import traceback
        global_logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

        # é™çº§åˆ°åŸæ¥çš„é€»è¾‘
        fallback_id = flow_data.get('app_id', 'unknown') + '_default_if'
        global_logger.warning(f"ä½¿ç”¨é™çº§ID: {fallback_id}")
        return fallback_id